@misc{wang2021automated,
title={Automated Concatenation of Embeddings for Structured Prediction},
author={Xinyu Wang and Yong Jiang and Nguyen Bach and Tao Wang and Zhongqiang Huang and Fei Huang and Kewei Tu},
year={2021},
url={https://openreview.net/forum?id=nCY83KxoehA}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}



@article{nadeau2007survey,
  abstract = {This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.},
  added-at = {2014-11-17T10:17:37.000+0100},
  author = {Nadeau, David and Sekine, Satoshi},
  biburl = {https://www.bibsonomy.org/bibtex/28da84a766f324af5f47a54409e65f551/jaeschke},
  doi = {10.1075/li.30.1.03nad},
  interhash = {2d9a1a5440885a8741a1686f344a9494},
  intrahash = {8da84a766f324af5f47a54409e65f551},
  journal = {Lingvisticae Investigationes},
  keywords = {classification entity named ner nerc nlp recognition survey},
  month = jan,
  number = 1,
  pages = {3--26},
  timestamp = {2014-11-18T09:32:51.000+0100},
  title = {A survey of named entity recognition and classification},
  url = {http://www.ingentaconnect.com/content/jbp/li/2007/00000030/00000001/art00002},
  volume = 30,
  year = 2007
}

@article {9039685,
author = {J. Li and A. Sun and J. Han and C. Li},
journal = {IEEE Transactions on Knowledge & Data Engineering},
title = {A Survey on Deep Learning for Named Entity Recognition},
year = {5555},
volume = {},
number = {01},
issn = {1558-2191},
pages = {1-1},
keywords = {deep learning;task analysis;tools;text recognition;annotations;encyclopedias},
doi = {10.1109/TKDE.2020.2981314},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {mar}
}

@inproceedings{lample-etal-2016-neural,
    title = "Neural Architectures for Named Entity Recognition",
    author = "Lample, Guillaume  and
      Ballesteros, Miguel  and
      Subramanian, Sandeep  and
      Kawakami, Kazuya  and
      Dyer, Chris",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1030",
    doi = "10.18653/v1/N16-1030",
    pages = "260--270",
}

@article{chiu-nichols-2016-named,
    title = "Named Entity Recognition with Bidirectional {LSTM}-{CNN}s",
    author = "Chiu, Jason P.C.  and
      Nichols, Eric",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://aclanthology.org/Q16-1026",
    doi = "10.1162/tacl_a_00104",
    pages = "357--370",
    abstract = "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",
}


@misc{huang2015bidirectional,
  abstract = {In this paper, we propose a variety of Long Short-Term Memory (LSTM) based
models for sequence tagging. These models include LSTM networks, bidirectional
LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer
(LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is
the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to
NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model
can efficiently use both past and future input features thanks to a
bidirectional LSTM component. It can also use sentence level tag information
thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or
close to) accuracy on POS, chunking and NER data sets. In addition, it is
robust and has less dependence on word embedding as compared to previous
observations.},
  added-at = {2018-11-27T10:12:17.000+0100},
  author = {Huang, Zhiheng and Xu, Wei and Yu, Kai},
  biburl = {https://www.bibsonomy.org/bibtex/217e5500fe201736ee2381c79517bcd0d/florianpircher},
  interhash = {e451a58a3f141b2b7d3ba99dcfff238f},
  intrahash = {17e5500fe201736ee2381c79517bcd0d},
  keywords = {},
  note = {cite arxiv:1508.01991},
  timestamp = {2018-11-27T10:12:17.000+0100},
  title = {Bidirectional LSTM-CRF Models for Sequence Tagging},
  url = {http://arxiv.org/abs/1508.01991},
  year = 2015
}


@misc{ma2016endtoend,
  abstract = {State-of-the-art sequence labeling systems traditionally require large
amounts of task-specific knowledge in the form of hand-crafted features and
data pre-processing. In this paper, we introduce a novel neutral network
architecture that benefits from both word- and character-level representations
automatically, by using combination of bidirectional LSTM, CNN and CRF. Our
system is truly end-to-end, requiring no feature engineering or data
pre-processing, thus making it applicable to a wide range of sequence labeling
tasks. We evaluate our system on two data sets for two sequence labeling tasks
--- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003
corpus for named entity recognition (NER). We obtain state-of-the-art
performance on both the two data --- 97.55\% accuracy for POS tagging and
91.21\% F1 for NER.},
  added-at = {2017-12-28T15:28:13.000+0100},
  author = {Ma, Xuezhe and Hovy, Eduard},
  biburl = {https://www.bibsonomy.org/bibtex/278235e1c3cf4f6744c1f22ca06564ad0/ykaitao},
  description = {[1603.01354] End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF},
  interhash = {f4a1276fdab99d6c2bad1d80e3db98fe},
  intrahash = {78235e1c3cf4f6744c1f22ca06564ad0},
  keywords = {Sequence labeling},
  note = {cite arxiv:1603.01354Comment: 10 pages, 3 figures. To appear on ACL 2016},
  timestamp = {2017-12-28T15:28:13.000+0100},
  title = {End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF},
  url = {http://arxiv.org/abs/1603.01354},
  year = 2016
}


@inproceedings{rei-2017-semi,
    title = "Semi-supervised Multitask Learning for Sequence Labeling",
    author = "Rei, Marek",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1194",
    doi = "10.18653/v1/P17-1194",
    pages = "2121--2130",
    abstract = "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.",
}

@misc{devlin2018pretraining,
  abstract = {We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations by jointly conditioning on both left and right
context in all layers. As a result, the pre-trained BERT representations can be
fine-tuned with just one additional output layer to create state-of-the-art
models for a wide range of tasks, such as question answering and language
inference, without substantial task-specific architecture modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI
accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question
answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human
performance by 2.0%.},
  added-at = {2019-02-05T23:35:51.000+0100},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  biburl = {https://www.bibsonomy.org/bibtex/210c860e3f390c6fbfd78a3b91ab9b0af/albinzehe},
  description = {[1810.04805] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  interhash = {a74f4c3853d3f0340e75546639134e91},
  intrahash = {10c860e3f390c6fbfd78a3b91ab9b0af},
  keywords = {bert elmo embeddings kallimachos nlp proposal-knowledge wordembeddings},
  note = {cite arxiv:1810.04805Comment: 13 pages},
  timestamp = {2020-07-28T14:17:24.000+0200},
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding},
  url = {http://arxiv.org/abs/1810.04805},
  year = 2018
}


@misc{liu2019roberta,
  abstract = {Language model pretraining has led to significant performance gains but
careful comparison between different approaches is challenging. Training is
computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the
final results. We present a replication study of BERT pretraining (Devlin et
al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can
match or exceed the performance of every model published after it. Our best
model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise
questions about the source of recently reported improvements. We release our
models and code.},
  added-at = {2020-12-11T12:52:54.000+0100},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  biburl = {https://www.bibsonomy.org/bibtex/2a4c60811a43da7596716d79b67d26e0a/marjaw},
  interhash = {040474bcd625e7dcc649bb20c81104d2},
  intrahash = {a4c60811a43da7596716d79b67d26e0a},
  keywords = {},
  note = {cite arxiv:1907.11692},
  timestamp = {2020-12-11T12:52:54.000+0100},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  url = {http://arxiv.org/abs/1907.11692},
  year = 2019
}


